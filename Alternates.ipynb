{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing dependecies\n",
    "import requests\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse, urlsplit\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Part Performing ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    def __init__(self, url=None):\n",
    "        self.url = url\n",
    "        self.domain = self.get_domain(url)\n",
    "        \n",
    "        self.weights={}\n",
    "        self.data={}\n",
    "        self.domains_visited = []\n",
    "\n",
    "\n",
    "        self.res={}\n",
    "        self.rows=[]\n",
    "\n",
    "        self.attributes = ['title', 'vendor', 'product_type', 'tags', 'handle']\n",
    "        \n",
    "\n",
    "\n",
    "    def validate_url(self):\n",
    "        try:\n",
    "            url = f'https://www.{self.domain}/collections/all/products.json?page=1'\n",
    "            products = requests.get(url).json()['products']\n",
    "            print(f\"Extracting data from the url:\", url)\n",
    "\n",
    "        except:\n",
    "            raise requests.HTTPError('Invalid url. Kindly check the same.')\n",
    "\n",
    "\n",
    "    def get_domain(self, url):\n",
    "\n",
    "        parsed_url = urlsplit(url)\n",
    "        domain= parsed_url.netloc[4:]\n",
    "\n",
    "\n",
    "        return domain\n",
    "    \n",
    "    def extract_data(self):\n",
    "\n",
    "        # extracting the data upto 5 pages\n",
    "\n",
    "        try:\n",
    "            for i in range(1,6):\n",
    "\n",
    "                # Send a request to the URL and get the page content\n",
    "                url = f'https://www.{self.domain}/collections/all/products.json?page={i}'\n",
    "                print(f\"Extracting data from the url:\", url)\n",
    "                products = requests.get(url).json()['products']\n",
    "                # content = response.content\n",
    "\n",
    "                # # Parse the HTML content using Beautiful Soup\n",
    "                # soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "                # # Find the products on the page\n",
    "                # products = soup.find_all('a', {'class': 'grid__image'})\n",
    "\n",
    "\n",
    "                for product in products:\n",
    "                    handle = product['handle']\n",
    "                    product_url = f'https://www.{self.domain}/products/{handle}.json'\n",
    "                    # print(f'{url=}')\n",
    "\n",
    "                    product_details = requests.get(product_url).json()['product']\n",
    "\n",
    "                    row = [product_details[i] for i in self.attributes]\n",
    "\n",
    "                    self.rows.append(row)\n",
    "\n",
    "            print(\"Data extraction successful.\")\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            raise requests.ConnectionError( 'Invalid url, Failed to establish a connection. Please check the url carefully.')\n",
    "            \n",
    "        \n",
    "    \n",
    "    def save_data(self):\n",
    "\n",
    "        \n",
    "\n",
    "        print('Transforming and Saving extracted data.')\n",
    "\n",
    "        try:\n",
    "\n",
    "            # converting into dataframe required in further operations\n",
    "            self.data = pd.DataFrame(data=self.rows, columns=self.attributes)\n",
    "        \n",
    "\n",
    "            # saving only the handles required for fetching the index of cosine matrix\n",
    "            handles = self.data[['handle']]\n",
    "\n",
    "            csv_path = f'data/{self.domain}.csv'\n",
    "            # create a directory for weights is doesn't exist \n",
    "            os.makedirs('data', exist_ok=True)\n",
    "\n",
    "\n",
    "            handles.to_csv(csv_path, index=False)\n",
    "            print(f'{csv_path} saved successfully.')\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(e)\n",
    "\n",
    "\n",
    "    def save_weights(self):\n",
    "\n",
    "        print('Calculating and saving import weight file.')\n",
    "\n",
    "        try:\n",
    "\n",
    "            # create a TF-IDF matrix\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            matrix = vectorizer.fit_transform(self.data['title'] + ' '+ self.data['vendor'] + ' ' + self.data['product_type'] + ' ' + self.data['tags'])\n",
    "\n",
    "            # get the cosine-similary as per the terms which are matching highest common\n",
    "            similarity = cosine_similarity(matrix)\n",
    "\n",
    "            weight_path = f'weights/{self.domain}.npy'\n",
    "            # create a directory for weights is doesn't exist \n",
    "            os.makedirs('weights', exist_ok=True)\n",
    "\n",
    "            np.save(weight_path, similarity)\n",
    "            print(f'{weight_path} saved successfully.')\n",
    "\n",
    "        except Exception as e:\n",
    "            raise ValueError(e)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def save(self):\n",
    "\n",
    "        try:\n",
    "\n",
    "            if  self.domain not in self.domains_visited:\n",
    "                print(\"Please wait while processing in the backend.\")\n",
    "\n",
    "                if not( os.path.exists(f'data/{self.domain}.csv') and os.path.exists(f'weights/{self.domain}.npy') ):\n",
    "                    print('Data doesn\\'t exist, it may take around a minute. Time may vary depends upon the network speed.')\n",
    "                    print(\"\\nStep #1/3\")\n",
    "                    self.extract_data()\n",
    "                    print(\"\\nStep #2/3\")\n",
    "                    self.save_data()\n",
    "                    print(\"\\nStep #3/3\")\n",
    "                    self.save_weights()\n",
    "\n",
    "                else:\n",
    "                    print('Data already exists.')\n",
    "                self.domains_visited.append(self.domain)\n",
    "\n",
    "\n",
    "            print(f'\\nAll necessary files is saved for {self.url}.\\n')\n",
    "\n",
    "            print('CONGRATULATIONS! You can proceed with 2nd Step.')\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "\n",
    "            return e\n",
    "\n",
    "\n",
    "            # print('Sorry, Unexpected error happened!!', e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1  Let's extract data for the domain 'https://www.boysnextdoor-apparel.co'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while processing in the backend.\n",
      "Data doesn't exist, it may take around a minute. Time may vary depends upon the network speed.\n",
      "\n",
      "Step #1/3\n",
      "Extracting data from the url: https://www.boysnextdoor-apparel.co/collections/all/products.json?page=1\n",
      "Extracting data from the url: https://www.boysnextdoor-apparel.co/collections/all/products.json?page=2\n",
      "Extracting data from the url: https://www.boysnextdoor-apparel.co/collections/all/products.json?page=3\n",
      "Extracting data from the url: https://www.boysnextdoor-apparel.co/collections/all/products.json?page=4\n",
      "Extracting data from the url: https://www.boysnextdoor-apparel.co/collections/all/products.json?page=5\n",
      "Data extraction successful.\n",
      "\n",
      "Step #2/3\n",
      "Transforming and Saving extracted data.\n",
      "data/boysnextdoor-apparel.co.csv saved successfully.\n",
      "\n",
      "Step #3/3\n",
      "Calculating and saving import weight file.\n",
      "weights/boysnextdoor-apparel.co.npy saved successfully.\n",
      "\n",
      "All necessary files is saved for https://www.boysnextdoor-apparel.co.\n",
      "\n",
      "CONGRATULATIONS! You can proceed with 2nd Step.\n"
     ]
    }
   ],
   "source": [
    "DataPrep('https://www.boysnextdoor-apparel.co').save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Try again extracting data for the same domain 'https://www.boysnextdoor-apparel.co'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while processing in the backend.\n",
      "Data already exists.\n",
      "\n",
      "All necessary files is saved for https://www.boysnextdoor-apparel.co.\n",
      "\n",
      "CONGRATULATIONS! You can proceed with 2nd Step.\n"
     ]
    }
   ],
   "source": [
    "DataPrep('https://www.boysnextdoor-apparel.co').save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Try an invalid url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait while processing in the backend.\n",
      "Data doesn't exist, it may take around a minute. Time may vary depends upon the network speed.\n",
      "\n",
      "Step #1/3\n",
      "Extracting data from the url: https://www.boysnextdoor-apparel.com/collections/all/products.json?page=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "requests.exceptions.ConnectionError('Invalid url, Failed to establish a connection. Please check the url carefully.')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: using '.com' instead of '.co'\n",
    "\n",
    "DataPrep('https://www.boysnextdoor-apparel.com').save()\n",
    "\n",
    "\n",
    "# Invalid URL Exception is thrown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Part : Testing the API\n",
    "\n",
    "\n",
    "### 2.1 Loading the previously processed data for the domain 'https://www.boysnextdoor-apparel.co'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self):\n",
    "        self.weights={}\n",
    "        self.data={}\n",
    "        self.domains_visited = []\n",
    "\n",
    "\n",
    "        self.res={}\n",
    "\n",
    "\n",
    "    def get_domain(self, url):\n",
    "        parsed_url = urlsplit(url)\n",
    "        domain= parsed_url.netloc[4:]\n",
    "\n",
    "        if domain not in self.domains_visited:\n",
    "            self.domains_visited.append(domain)\n",
    "\n",
    "\n",
    "        return domain\n",
    "\n",
    "\n",
    "    def get_item(self, url):\n",
    "\n",
    "        item = url.split('/')[-1]\n",
    "\n",
    "        item = re.sub(r'\\.json$', '', item)\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "\n",
    "    def get_data(self,domain):\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "            if domain not in self.data:\n",
    "                self.data[domain] = pd.read_csv(f'data/{domain}.csv')\n",
    "\n",
    "            print('data loading success')\n",
    "            return self.data[domain]\n",
    "        \n",
    "        except:\n",
    "            raise OSError( f'data/{domain}.csv doesn\\'t exist.')\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def get_weight(self, domain):\n",
    "\n",
    "\n",
    "        try:\n",
    "            if domain not in self.weights:\n",
    "                self.weights[domain] = np.load(f'weights/{domain}.npy')\n",
    "\n",
    "            print('weight loading success')\n",
    "            return self.weights[domain]\n",
    "        \n",
    "\n",
    "        except:\n",
    "            raise OSError ( f'weights/{domain}.npy doesn\\'t exist.')\n",
    "    \n",
    "\n",
    "    def get_most_similarity_idx(self, product_similarity):\n",
    "        similarity = sorted(product_similarity, reverse=True)\n",
    "\n",
    "        threshold = similarity[0] - similarity[1] + 0.07\n",
    "\n",
    "        for i in range(1, 10):\n",
    "            diff = similarity[0] - similarity[i]\n",
    "            \n",
    "            if diff > threshold:\n",
    "                break\n",
    "        return i\n",
    "\n",
    "\n",
    "    def FindAlternateGroups(self, url):\n",
    "        item = self.get_item(url)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # optimization to prevent re-processing for the same item\n",
    "            if item in self.res:\n",
    "                return self.res[item]\n",
    "\n",
    "            print(f'{item=}')\n",
    "\n",
    "            domain = self.get_domain(url)\n",
    "\n",
    "            print(f'{domain=}')\n",
    "\n",
    "            data = self.get_data(domain)\n",
    "\n",
    "            \n",
    "            try:\n",
    "                idx = data[data['handle'] == item].index[0]\n",
    "\n",
    "            except:\n",
    "                raise ValueError (f'sorry this item `{item}` is not taken into consideration')\n",
    "\n",
    "            \n",
    "\n",
    "            similarity = self.get_weight(domain)\n",
    "\n",
    "            self.product_similarity = similarity[idx]\n",
    "\n",
    "            # sort the similarity scores in descending order\n",
    "            self.similar_indices = self.product_similarity.argsort()[::-1]\n",
    "\n",
    "        \n",
    "\n",
    "            # get the elements whose difference with the original item is under a certain threshold\n",
    "            self.most_sim_idx = self.get_most_similarity_idx(self.product_similarity)\n",
    "            # generate recommendations\n",
    "            recommendations = self.similar_indices[:self.most_sim_idx]\n",
    "\n",
    "            \n",
    "\n",
    "            res = data.iloc[recommendations].copy()\n",
    "\n",
    "\n",
    "            output_url = re.sub(item, '', url)\n",
    "\n",
    "            res['handle'] = (output_url + res['handle'])\n",
    "\n",
    "            final_res = {\n",
    "                'product alternates': list(res['handle'].values)\n",
    "            }\n",
    "\n",
    "\n",
    "            self.res[item] = final_res\n",
    "\n",
    "\n",
    "            return final_res\n",
    "        \n",
    "        except Exception as e:\n",
    "\n",
    "            return e\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DataPrep('https://www.boysnextdoor-apparel.co').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataloader initialization\n",
    "solution = DataLoader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Testing the API with a product url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item='beams-two-pocket-sweater-navy'\n",
      "domain='boysnextdoor-apparel.co'\n",
      "data loading success\n",
      "weight loading success\n",
      "{'product alternates': ['https://www.boysnextdoor-apparel.co/products/beams-two-pocket-sweater-navy', 'https://www.boysnextdoor-apparel.co/products/beams-two-pocket-sweater-white']}\n"
     ]
    }
   ],
   "source": [
    "name = 'beams-two-pocket-sweater-navy' \n",
    "item_url = f'https://www.boysnextdoor-apparel.co/products/{name}'\n",
    "\n",
    "res = solution.FindAlternateGroups(item_url)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testing another product url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item='ben-davis-heavy-duty-pocket-tee-black'\n",
      "domain='boysnextdoor-apparel.co'\n",
      "data loading success\n",
      "weight loading success\n",
      "{'product alternates': ['https://www.boysnextdoor-apparel.co/products/ben-davis-heavy-duty-pocket-tee-black', 'https://www.boysnextdoor-apparel.co/products/ben-davis-heavy-duty-pocket-tee-white', 'https://www.boysnextdoor-apparel.co/products/ben-davis-pocket-l-s-tee-black', 'https://www.boysnextdoor-apparel.co/products/ben-davis-heavy-duty-pocket-tee-charcoal', 'https://www.boysnextdoor-apparel.co/products/ben-davis-heavy-duty-pocket-tee-ash-grey']}\n"
     ]
    }
   ],
   "source": [
    "name = 'ben-davis-heavy-duty-pocket-tee-black' \n",
    "item_url = f'https://www.boysnextdoor-apparel.co/products/{name}'\n",
    "\n",
    "res = solution.FindAlternateGroups(item_url)\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Testing the api again with incorrect product url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'product alternates': ['https://www.boysnextdoor-apparel.co/products/beams-two-pocket-sweater-navy', 'https://www.boysnextdoor-apparel.co/products/beams-two-pocket-sweater-white']}\n"
     ]
    }
   ],
   "source": [
    "name = 'beams-two-pocket-sweater-navy' \n",
    "item_url = f'https://www.boysnextdoor-apparel.com/products/{name}'\n",
    "\n",
    "# NOTE: '.com' used instead of '.co' in the domain\n",
    "\n",
    "res = solution.FindAlternateGroups(item_url)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Testing the api once more with incorrect product url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item='beams-two-pocket-sweater-navy-blue'\n",
      "domain='boysnextdoor-apparel.co'\n",
      "data loading success\n",
      "sorry this item `beams-two-pocket-sweater-navy-blue` is not taken into consideration\n"
     ]
    }
   ],
   "source": [
    "name = 'beams-two-pocket-sweater-navy-blue' \n",
    "item_url = f'https://www.boysnextdoor-apparel.co/products/{name}'\n",
    "\n",
    "# NOTE: 'navy-blue' used instead of 'navy' in the product name\n",
    "\n",
    "res = solution.FindAlternateGroups(item_url)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
